# Applying Segment Level Attention on Bi-Modal Transformer Encoder for Audio-Visual Emotion Recognition

!["our proposed system frameworks"](https://github.com/Xuplussss/Applying-Segment-Level-Attention-on-Bi-Modal-Transformer-Encoder-for-Audio-Visual-EmotionRecognition/blob/main/SystemFrameworks.png?raw=true)

## Requirements
- Python >= 3.7
- transformers >= 4.24.0
- PyTorch >= 1.13.0
- pytorch-lightning

## Audio feature extraction model
```
cd audio_feature_extraction
```

## Facial feature extraction model
```
cd visual_feature_extraction
```

## Fusion & emotion recognition
```
cd Bimodal_Transformer_encoder
```

## Reference
This package provides training code for the audio-visual emotion recognition paper. If you use this codebase in your experiments please cite: 

```
@article{hsu2023applying,
  title={Applying Segment-Level Attention on Bi-modal Transformer Encoder for Audio-Visual Emotion Recognition},
  author={Hsu, Jia-Hao and Wu, Chung-Hsien},
  journal={IEEE Transactions on Affective Computing},
  year={2023},
  publisher={IEEE}
}
```
